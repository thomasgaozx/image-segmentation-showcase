{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqtaBfMeM-7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff61f4a-c7b2-4ca0-96aa-f6a62735bc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/segmentation/segmentation.py:8: UserWarning: The 'torchvision.models.segmentation.segmentation' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.models.segmentation' directly instead.\n",
            "  \"The 'torchvision.models.segmentation.segmentation' module is deprecated since 0.12 and will be removed in \"\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import numbers\n",
        "import platform\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import Tensor\n",
        "from torch.jit import script, trace\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as tF\n",
        "\n",
        "#device = torch.device('cuda')\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# img = cv2.imread('data/0/frame_1.jpg', cv2.IMREAD_COLOR)\n",
        "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "# plt.axis('off')\n",
        "# plt.imshow(img)\n",
        "\n",
        "# def draw_rect_perm(image, arr):\n",
        "#     top_left, bottom_right = (arr[0], arr[1]), (arr[0]+arr[2], arr[1] + arr[3])\n",
        "#     cv2.rectangle(image, top_left, bottom_right, (255,0,0), 2)\n",
        "\n",
        "# roi = [[502, 344, 61, 86],\n",
        "#       [627, 427, 193, 93],\n",
        "#       [522, 654, 90, 85],\n",
        "#       [704, 594, 90, 80],\n",
        "#       [545, 577, 81, 75],\n",
        "#       [460, 700, 52, 80]]\n",
        "\n",
        "# def show_roi(image):\n",
        "#     img2 = image.copy()\n",
        "#     for i in range(6):\n",
        "#         draw_rect_perm(img2, roi[i])\n",
        "#     draw_rect_perm(img2, [400, 300, 500, 500])\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(img2)\n",
        "\n",
        "# show_roi(img)\n",
        "# plt.show()\n",
        "# exit()\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.segmentation.segmentation import fcn_resnet101\n",
        "from torchvision.io.image import read_image\n",
        "from torchvision.models.segmentation import fcn_resnet50, deeplabv3_resnet50\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def crop500(image):\n",
        "    return tF.crop(image, 300, 400, 500, 500)\n",
        "\n",
        "# i = 0\n",
        "# def save(imgs):\n",
        "#     global i\n",
        "#     if not isinstance(imgs, list):\n",
        "#         imgs = [imgs]\n",
        "#     for _, img in enumerate(imgs):\n",
        "#         img = img.detach()\n",
        "#         save_image(img, f't{i}.jpg')\n",
        "#         i += 1\n",
        "\n",
        "# resize = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Lambda(crop500),\n",
        "#     #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "#     ])\n",
        "# dataset = torchvision.datasets.ImageFolder('shiti/', transform=resize)\n",
        "# dataloader = DataLoader(dataset, batch_size=5)\n",
        "# for data, _ in dataloader:\n",
        "#     save([data[i] for i in range(data.shape[0])])\n",
        "# # exit()\n",
        "\n",
        "# def cropCUS(image):\n",
        "#     return tF.crop(image, 344, 502, 61, 86)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#img = read_image('/content/data/frame_1.jpg')\n",
        "# resize = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Lambda(crop500),\n",
        "#     #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "#     ])\n",
        "# dataset = torchvision.datasets.ImageFolder('data/', transform=resize)\n",
        "# dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "\n",
        "# model = fcn_resnet50(pretrained=True, progress=False)\n",
        "# model = model.eval()\n",
        "# model = model.to(device)\n",
        "\n",
        "# for data, _ in dataloader:\n",
        "#     data = data.to(device)\n",
        "#     out = model(data)['out']\n",
        "#     print(model(data)['aux'].shape)\n",
        "#     print(model(data)['out'].shape)\n",
        "#     break\n",
        "\n",
        "# #normalized_batch = F.normalize()\n",
        "# sem_classes = [\n",
        "#     '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "#     'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "#     'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
        "# ]\n",
        "# sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\n",
        "# normalized_masks = torch.nn.functional.softmax(out, dim=1)\n",
        "\n",
        "# human_mask = [normalized_masks[img_idx, sem_class_to_idx['person']]\n",
        "#     for img_idx in range(data.shape[0])]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = h = 500\n",
        "batch_size = 8\n",
        "train_dir = \"/content/shitout\"\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "transformAnn = transforms.ToTensor()\n",
        "\n",
        "\n",
        "batches = []\n",
        "for i in range(batch_size):\n",
        "    img = cv2.imread(f\"/content/shitout/t{i}.jpg\")\n",
        "    lbl = cv2.imread(f\"/content/verify/t{i}.jpg\")\n",
        "\n",
        "    ann_map = np.zeros(img.shape[0:2], bool)\n",
        "    for i in range(500):\n",
        "        for j in range(500):\n",
        "          ann_map[i,j] = np.any(lbl[i,j] < 250)\n",
        "    #plt.imshow(ann_map)\n",
        "    img = transform(img)\n",
        "    ann_map = transformAnn(ann_map)\n",
        "    batches.append((img,ann_map))\n",
        "\n",
        "def load_batch():\n",
        "    imgs = torch.zeros([batch_size,3,h,w])\n",
        "    ann = torch.zeros([batch_size,h,w])\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      imgs[i], ann[i] = batches[i][0], batches[i][1]\n",
        "\n",
        "    return imgs, ann\n"
      ],
      "metadata": {
        "id": "Pl6rmY4dhukI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = deeplabv3_resnet50(pretrained=True)\n",
        "model.classifier[4] = nn.Conv2d(256, 2, kernel_size=(1,1), stride=(1,1))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "iZwpn3wDo5tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### retrain model\n",
        "lr = 1e-4\n",
        "\n",
        "\n",
        "# first train the last layer\n",
        "optimizer = torch.optim.Adam(model.classifier[4].parameters(),\n",
        "                             lr=lr)\n",
        "\n",
        "model.train()\n",
        "for iter in range(500):\n",
        "    imgs,ann = load_batch()\n",
        "    optimizer.zero_grad()\n",
        "    with torch.set_grad_enabled(True):\n",
        "        imgs = torch.autograd.Variable(imgs,requires_grad=False).to(device)\n",
        "        ann = torch.autograd.Variable(ann, requires_grad=False).to(device)\n",
        "        pred = model(imgs)['out']\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        loss=criterion(pred,ann.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if iter % 10 == 0:\n",
        "        print(loss.item())\n",
        "\n",
        "\n",
        "    #running_loss.append(loss.item())      \n",
        "\n",
        "torch.save(model.state_dict(), \"naive0.torch\")\n",
        "\n",
        "# then fine tune the entire model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OId7JDfpYi0",
        "outputId": "318e6a67-d959-404d-dade-fa54d7cffb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5312653183937073\n",
            "0.47667768597602844\n",
            "0.42740321159362793\n",
            "0.38468295335769653\n",
            "0.34687575697898865\n",
            "0.3145785331726074\n",
            "0.286141037940979\n",
            "0.26163703203201294\n",
            "0.24040980637073517\n",
            "0.2216147929430008\n",
            "0.20556199550628662\n",
            "0.19153280556201935\n",
            "0.1788620948791504\n",
            "0.16748961806297302\n",
            "0.15755106508731842\n",
            "0.14868180453777313\n",
            "0.14084526896476746\n",
            "0.13375510275363922\n",
            "0.12776906788349152\n",
            "0.12151747196912766\n",
            "0.11636745929718018\n",
            "0.11171677708625793\n",
            "0.10690564662218094\n",
            "0.10316788405179977\n",
            "0.09963300079107285\n",
            "0.09633109718561172\n",
            "0.09339546412229538\n",
            "0.09027513861656189\n",
            "0.08748038858175278\n",
            "0.08522737771272659\n",
            "0.08341772109270096\n",
            "0.0810476541519165\n",
            "0.0791003480553627\n",
            "0.07691626250743866\n",
            "0.07552483677864075\n",
            "0.07418457418680191\n",
            "0.07277900725603104\n",
            "0.07093112170696259\n",
            "0.06987183541059494\n",
            "0.06870714575052261\n",
            "0.06758219003677368\n",
            "0.06613949686288834\n",
            "0.06563197076320648\n",
            "0.06436093896627426\n",
            "0.06338363885879517\n",
            "0.06238926947116852\n",
            "0.061885423958301544\n",
            "0.06075203791260719\n",
            "0.060325298458337784\n",
            "0.059818923473358154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = deeplabv3_resnet50(pretrained=True)\n",
        "model.classifier[4] = nn.Conv2d(256, 2, kernel_size=(1,1), stride=(1,1))\n",
        "model.load_state_dict(torch.load(\"naive1.torch\",map_location=torch.device('cpu')))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "jkYnPECL1jM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a144d57-2348-412e-fa26-cf390486a52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "lr = 1e-5\n",
        "\n",
        "\n",
        "# first train the last layer\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=lr)\n",
        "\n",
        "model.train()\n",
        "for iter in range(150):\n",
        "    imgs,ann = load_batch()\n",
        "    optimizer.zero_grad()\n",
        "    with torch.set_grad_enabled(True):\n",
        "        imgs = torch.autograd.Variable(imgs,requires_grad=False).to(device)\n",
        "        ann = torch.autograd.Variable(ann, requires_grad=False).to(device)\n",
        "        pred = model(imgs)['out']\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        loss=criterion(pred,ann.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if iter % 10 == 0:\n",
        "        print(loss.item())\n",
        "\n",
        "\n",
        "    #running_loss.append(loss.item())      \n",
        "\n",
        "torch.save(model.state_dict(), \"naive1.torch\")\n",
        "\n",
        "# then fine tune the entire model"
      ],
      "metadata": {
        "id": "nF0acN6eN6kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd91fe5-8b7c-4ace-9685-cfd06249cd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06729988753795624\n",
            "0.048897214233875275\n",
            "0.04241541028022766\n",
            "0.03817346319556236\n",
            "0.0354403592646122\n",
            "0.033113740384578705\n",
            "0.03131159022450447\n",
            "0.029709307476878166\n",
            "0.02817200869321823\n",
            "0.026802441105246544\n",
            "0.025583485141396523\n",
            "0.024541478604078293\n",
            "0.023612130433321\n",
            "0.022763174027204514\n",
            "0.021989120170474052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23FlUzbw3FhQ",
        "outputId": "ff538aad-c3e5-4a7f-c92a-7c25b2277a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = tF.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img), aspect='auto')\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n"
      ],
      "metadata": {
        "id": "t5j7KJotAFCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(crop500),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "dataset = torchvision.datasets.ImageFolder('data/', transform=resize)\n",
        "dataloader = DataLoader(dataset, batch_size=5)\n",
        "\n",
        "for data, _ in dataloader:\n",
        "    data = data.to(device)\n",
        "    out = model(data)['out']\n",
        "    break\n",
        "\n",
        "normalized_masks = torch.nn.functional.softmax(out, dim=1)\n",
        "human_mask = [normalized_masks[i, 1] for i in range(data.shape[0])]\n",
        "\n",
        "\n",
        "# #print(normalized_masks[0])\n",
        "\n",
        "\n",
        "resize = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(crop500),\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "dataset = torchvision.datasets.ImageFolder('data/', transform=resize)\n",
        "dataloader = DataLoader(dataset, batch_size=5)\n",
        "\n",
        "for data, _ in dataloader:\n",
        "    data = data.to(device)\n",
        "    break\n",
        "show([data[img_idx] for img_idx in range(data.shape[0])])\n",
        "show(human_mask)"
      ],
      "metadata": {
        "id": "BN4rLngsxaXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 20\n",
        "\n",
        "roi = [[502, 344, 61, 86],\n",
        "      [627, 427, 193, 93],\n",
        "      [522, 654, 90, 85],\n",
        "      [704, 594, 90, 80],\n",
        "      [545, 577, 81, 75],\n",
        "      [460, 700, 52, 80]]\n",
        "\n",
        "#[400, 300, 500, 500]\n",
        "\n",
        "region = [np.zeros((500,500), dtype=bool) for _ in range(6)]\n",
        "for i in range(6):\n",
        "    a = roi[i][0]-400\n",
        "    b = roi[i][1]-300\n",
        "    region[i][b:b+roi[i][3], a:a+roi[i][2]] = 1\n",
        "\n",
        "# for i in range(1, 13502):\n",
        "#     img = f\"/content/drive/MyDrive/musashi_challenge/frame_{i}.jpg\"\n",
        "\n",
        "batches = []\n",
        "model.eval()\n",
        "# for i in range(1, 13502):\n",
        "#     img = cv2.imread(f\"/content/drive/MyDrive/musashi_challenge/frame_{i}.jpg\")\n",
        "#     #plt.imshow(ann_map)\n",
        "#     img = resize(img)\n",
        "#     print(img.shape)\n",
        "#     tmp = torch.zeros([1,3,h,w])\n",
        "#     tmp[0] = img\n",
        "#     img = tmp\n",
        "#     out = model(img)['out']\n",
        "\n",
        "#     normalized_masks = torch.nn.functional.softmax(out, dim=1)\n",
        "#     human_mask = normalized_masks[0, 1]\n",
        "#     print(normalized_masks.shape)\n",
        "#     print(human_mask.shape)\n",
        "#     show(human_mask)\n",
        "#     break;\n",
        "\n",
        "for i in range(1, 13502):\n",
        "    img = cv2.imread(f\"/content/drive/MyDrive/musashi_challenge/frame_{i}.jpg\")\n",
        "    #plt.imshow(ann_map)\n",
        "    img = resize(img)\n",
        "    print(img.shape)\n",
        "    tmp = torch.zeros([1,3,h,w])\n",
        "    tmp[0] = img\n",
        "    img = tmp\n",
        "    out = model(img)['out']\n",
        "\n",
        "    normalized_masks = torch.nn.functional.softmax(out, dim=1)\n",
        "    human_mask = normalized_masks[0, 1]\n",
        "    print(normalized_masks.shape)\n",
        "    print(human_mask.shape)\n",
        "    show(human_mask)\n",
        "    break;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "SgrXcZYF3RoW",
        "outputId": "1dbabab0-c80b-41ba-ffc7-623fe8afe498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-19862d1c902f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/content/drive/MyDrive/musashi_challenge/frame_{i}.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#plt.imshow(ann_map)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pic should be PIL Image or ndarray. Got {type(pic)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_numpy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'NoneType'>"
          ]
        }
      ]
    }
  ]
}